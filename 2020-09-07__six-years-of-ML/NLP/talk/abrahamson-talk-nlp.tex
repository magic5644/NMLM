\documentclass[t,aspectratio=169]{beamer}

\input preamble

\title
{NMLM}
\subtitle{NLP}

% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command: 
%\beamerdefaultoverlayspecification{<+->}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Disclaimer}
  \textbf{We have 13 minutes for 6 years.}

  This will simplify quite a lot.  \blue{Maybe too much.  The goal
    is to trace the evolution of techniques and to give hints about
    why.}

  \red{I'm trying to avoid the non-essential stuff, like optimisation
  improvements that aren't core to why the state of the art advanced.}

  All the papers are on github.
\end{frame}

\begin{frame}{Overview}
  What's changed:
  \begin{itemize}
  \item (Much) better performance
  \item Polysemy
  \item Pre-trained models
  \end{itemize}
\end{frame}

\begin{frame}{Overview}
  \begin{itemize}
  \item $\le 2014$
  \item 2014: word2vec and improvements
    \only<1>{
  \item 2018: ELMo
  \item 2019: BERT
  \item 2018--2020: GPT
    }
    \only<2>{
  \item \blue{2018: ELMo}
  \item \blue{2019: BERT}
  \item \blue{2018--2020: GPT}
    }
  \end{itemize}
\end{frame}

\begin{frame}{Overview}
  Pre-trained models:
  \begin{itemize}
  \item Context-free \gray{(word2vec, FastText, GloVe)}
  \item Contextual
    \begin{itemize}
    \item Unidirectional \gray{(ELMo (I think))}
    \item Bidirectional \gray{(BERT, GPT-2,3 (I think))}
    \end{itemize}
  \end{itemize}

  \vspace{1cm}
  \gray{\it The literature talks about uni- and bidirectional but isn't
    clear about which algorithms are which.  This is my
    interpretation, and I might be wrong.  I'm not an NLP specialist.}
\end{frame}

\begin{frame}{Before 2014}
  TF-IDF (classic example)

  \begin{itemize}
  \item 1-hot encoding to get high-dimensional vectors
  \item TF-IDF to scale (weight)
  \only<2->{\item PCA or t-SNE etc. (if we want lower dimensionality)}
  \end{itemize}

  \only<3>{\blue{
  What do we have?
  \begin{itemize}
  \item \blue{A space of likely significant words.}
  \item \blue{Likely no good semantic knowledge.}
  \end{itemize}
}}

\only<1>{
  \bigskip
  Goal: classify documents (or phrases).
  }
\end{frame}

\begin{frame}{Word2vec}
  \begin{itemize}
  \item NN, but not deep: think auto-encoder or RBL
  \item Train on context: similar context $\Rightarrow$ similar embedding
  \item Want: distortion $\approx$ meaning 
  \end{itemize}

  \only<2->{Mikolov: trade complexity for efficiency $\Rightarrow$
    learn bigger datasets.}
  
  \only<3>{
    \bigskip
    \blue{Uses: search, translation, \dots}}

  \only<3>{\prevwork{Tomas Mikolov et al., Efficient Estimation of Word Representations in
    Vector Space, ICLR Workshop, 2013.}}
\end{frame}

\begin{frame}{Word2vec}
  Encode words at learning time.

  \bigskip
  \centerline{\red{($\R^N \rightarrow \R^n \quad$
      \hspace{2mm}for $N\approx 5\times 10^5$ 
      \hspace{2mm}and\hspace{2mm} $n\approx 300$)}}
  \bigskip

  Problem: no polysemy

\end{frame}

\begin{frame}{Word2vec}
  How does it work?

  It's learning, so we're optimising something.  What?

  \only<2>{\vspace{7mm}\centerline{\blue{similarity of context}}}
  \only<3->{\vspace{7mm}\centerline{\blue{cosine distance for similar contexts}}}

  \only<4>{ Look at words around the target word.  Think of as an
    n-gram.  We want words with similar context to project close
    together, words with different context (ideally) not to.  }
\end{frame}

\begin{frame}{Word2vec}
  Interesting properties:

  \begin{itemize}
  \item Magnitude is related to importance
    \begin{itemize}
    \item Too common: rarely learns large magnitude
    \item Too rare: rarely grows large
    \item So Goldilocks property (mid-range is just right)
    \end{itemize}
    \only<2>{\vspace{2mm}\item \blue{Learns stop words.}  Because their contexts are
      uncorrelated, they optimise near zero.}
  \end{itemize}
\end{frame}

\begin{frame}{Word2vec}
  How does it work?

  \only<1>{
    It's not a single algorithm, but presented in multiple variants
    (``efficiency improvements'').  Two notable elements
    (architectures) are
    \begin{itemize}
    \item CBOW (continuous bag-of-words) -- context predicts word
    \item SG (skipgram) -- word predicts context \gray{(SGNS =
        skipgram  negative sampling)}
    \end{itemize}

    \bigskip
    \prevwork{Tomas Mikolov et al., Distributed Representations of Words
      and Phrases and their Compositionality, NIPS 2013.}
  }
  \only<2>{
    \begin{itemize}
    \item SGNS factorises a word-context PMI matrix \gray{(This is not
        the most important take-away, but to point out that the
        mathematical foundations are slowly being understood.)}
    \end{itemize}

    \bigskip

    PMI = pointwise mutual information
    \begin{displaymath}
      pmi(x;y) = \log\left(\frac{\pr{x,y}}{\pr{x} \,\pr{y}}\right)
      = \log\left(\frac{\pr{x\mid y}}{\pr{x}}\right)
      = \log\left(\frac{\pr{y\mid x}}{\pr{y}}\right)
    \end{displaymath}

    \bigskip \prevwork{Sanjeev Arora et al., A latent variable model
      approach to word embeddings, Trans. Assoc. Comput. Linguistics
      4: 385-399 (2016).}
  }
  \only<3-4>{ Output is a softmax: cost is
    proportional to number of classes (50K words).
  }
  \only<4>{

    \bigskip
    \blue{Approximate the softmax to reduce cost: \gray{hierarchical
        softmax: $O(N) \Rightarrow O(\log n)$.}}

    \bigskip
    \prevwork{Yoav Goldber and Omber Levy, Word2vec Explained, arXiv 2014.}
  }
\end{frame}

\begin{frame}{Word2vec}
  \only<1>{
    The famous ``man : woman as king : queen'' in maths: we're
    maximising two similarities and minimising a dissimilarity.

    \bigskip
    But remember that we're really working with cosine similarity.
  }
  \only<2>{
    \resizebox{.9\textwidth}{!}{\input{vector.pdf_t}}
  }
\end{frame}

\begin{frame}{Word2vec}
  More references (hardly exhaustive):
  \begin{itemize}
  \item \prevwork{Yann LeCun, Bengio, Hinton, Deep Learning, Nature
      521, 436--444(2015).}
  \item \prevwork{Omer Levy and Yoav Goldberg, Neural Word Embedding
      as Implicit Matrix Factorization, 2014.}
  \item \prevwork{Omer Levy and Yoav Goldberg, Linguistic Regularities
      in Sparse and Explicit Word Representations, 2014.}
  \item \prevwork{Omer Levy et al., A Strong Baseline for Learning
      Cross-Lingual Word Embeddings from Sentence Alignments, 2017.}
  \end{itemize}
\end{frame}

\begin{frame}{Word2vec}
  Improvements:
  \begin{enumerate}
  \item FastText (Facebook)
    \only<1>{

      \bigskip
      \prevwork{Piotr Bojanowski et al., Enriching Word Vectors
        with Subword Information, 2017.}

      \bigskip
      \prevwork{Armand Joulin, Bag of Tricks for Efficient Text
        Classification, 2016.}
    }
    \only<2->{
    \item GloVe (``Global Vectors'') % unsupervised, distance \similar
      % semantic similarity, training on
      % word-word co-occurance matrix --
      % global matrix factorisation + local
      % context-window methods
    }
    \only<2>{
      
      \bigskip
      \prevwork{Jeffrey Pennington et al., GloVe: Global Vectors for
        Word Representation, 2014.}

      \bigskip
      \prevwork{\url{https://nlp.stanford.edu/projects/glove/}}
    }
    \only<3->{
    \item ULMFit (``Universal Language Model Fine Tuning'')
    }
    \only<3>{
      
      \bigskip
      \prevwork{Jeremy Howard and Sebastian Ruder, Universal Language
        Model Fine-tuning for Text Classification, 2018.}

      \bigskip
      \prevwork{Andrew Dai and Quoc Le, Semi-supervised Sequence
        Learning, 2015.}  \textit{($\leftarrow$ not ULMFiT, actually)}
    }
  \end{enumerate}

  \only<4->{
    Better performance, similar properties.
    
    What NLP was last quarter century, just a lot better.
  }
  \only<5>{

    \bigskip
    \textbf{\red{Think twice before using any of these, they're mostly obsolete.}}
  }
\end{frame}

\begin{frame}{ELMo}
  \cimg{../elmo/elmo.jpg}
\end{frame}

\begin{frame}{ELMo}
  \begin{itemize}
  \item Encode words before and after \gray{(not just at learning
      time)}
    \begin{itemize}
    \item \red{No:} dictionary: map word to vector
    \item \textbf{Yes:} on the fly, run context through deep network to produce
      new context
    \end{itemize}
  \item Captures linguistic context: polysemy
  \item Models word use: captures both syntactic and semantic
    information
  \item Most tasks better than word2vec and relatives \gray{(question
    answering, named entity exceptions, sentiment analysis, \dots)}
  \end{itemize}
\end{frame}

\begin{frame}{ELMo}
  \begin{itemize}
  \item Bidirectional LSTM + residual connectors between 1st and
    second layer
  \item Character embedding rather than word: helps with
    out-of-vocabular words
  \item Convolutional filters instead of n-gram features
  \end{itemize}

  \only<1>{Up to here, similar to Jozfowicz

    \prevwork{Rafal Jozfowicz, Exploring the Limits of Language
      Modeling, 2016.}
  }
  \only<2>{
    \begin{itemize}
    \item Learn different representations for different tasks
    \item Transfer learning
    \end{itemize}

    \blue{In CV, it's standard to learn ImageNet and transfer to problem at
    hand.}

    \blue{ELMo shows we can do this for NLP problems.}
  }
\end{frame}

\begin{frame}{ELMo}
  \cimghhh{../elmo/2layer-bidir-LSTM.png}
  \vspace{-5mm}
  \prevwork{\url{https://www.mihaileric.com/posts/deep-contextualized-word-representations-elmo/}}
\end{frame}

\begin{frame}{ELMo}
  \cimghhh{../elmo/transformation.png}
  \vspace{-15mm}
  \prevwork{\url{https://www.mihaileric.com/posts/deep-contextualized-word-representations-elmo/}}
\end{frame}

\begin{frame}{ELMo}
  See also
  \begin{itemize}
  \item \prevwork{Matt Gardner et al., AllenNLP: A Deep Semantic
      Natural Language Processing Platform}
  \item \prevwork{Yoon Kim, Character-Aware Neural Language Models,
      2015.}
  \item \prevwork{Matthew Peters et al., Deep contextualized word
      representations, 2018.}
  \item \prevwork{Rupesh Kumar Srivastava et al., Highway Networks, 2015.}
  \end{itemize}
\end{frame}

\begin{frame}{BERT}
  \cimg{../bert/bert.png}
\end{frame}

\begin{frame}{BERT}
  \begin{itemize}
  \item Builds on ELMo and first GPT
  \item Encoder-decoder: use self-attention to encode and attention to decode
  \end{itemize}

  \prevwork{Jacob Devlin et al., BERT: Pre-training of Deep Bidirectional Transformers for
    Language Understanding, 2019.}

  \prevwork{Ashish Vaswani, Attention is All You Need, 2017.}
\end{frame}

\begin{frame}{BERT}
  \cimg{../bert/BERT-example1.jpg}
\end{frame}

\begin{frame}{BERT}
  \cimg{../bert/BERT-example2.jpg}
\end{frame}

\begin{frame}{GPT}
  Generative Pre-trained Transformer

  \begin{itemize}
  \item The first GPT (6/2018) was in the ELMo/BERT world (\textit{very}
    roughly).  It demonstrated real-world knowledge acquisition and
    long-range dependencies.
    \only<2->{
    \item GPT-2 and GPT-3 distinguished themselves by scaring people.
    }
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\talksection{Break}

\begin{frame}
  \vphrase{\large Questions?}
\end{frame}

\end{document}
